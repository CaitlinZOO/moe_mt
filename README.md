<div align="center">
  <h1>LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training</h1>
  <img src="docs/imgs/title-favicon.png" width="200" alt="LLaMA-MoE favicon" style="border-radius: 5%;"><br />
  <span style="color:red">ğŸ“¢ <strong><i>A SMALLER AFFORDABLE MoE MODEL FOR EVERYONE!!</i></strong></span>
  <div>
    <a href="https://huggingface.co/LLaMA-MoE-v2" target="_blank">ğŸ¤— Model Weights</a> | <a href="#quick-start">ğŸš€ Quick Start</a> | <a href="#installation">âš™ï¸ Installation Guide</a> | <a href="#expert-construction">ğŸš§ Expert Construction</a> | <a href="#sft">ğŸ’¬ Supervised Fine-Tuning (SFT)</a> | <a href="#evaluation">ğŸ’ Evaluation</a>  <br /> 
    <a href="https://arxiv.org/pdf/2411.15708" target="_blank" style="display: inline-block; margin-top: 10px;"> ğŸ“ƒ Technical Report </a>
  </div>
</div>



<h2 id="quick-start">ğŸš€ QuickStart</h2>

```python
# python>=3.10

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_dir = "LLaMA-MoE-v2/LLaMA-MoE-v2-3_5B-2_8"
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16, trust_remote_code=True)
model.eval()
model.to("cuda:0")

input_text = "Suzhou is famous for?"

input_text = f"<|start_header_id|>user<|end_header_id|>\n\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

inputs = tokenizer(input_text, return_tensors="pt")
inputs = inputs.to("cuda:0")

pred = model.generate(**inputs, max_length=50, temperature=0.0)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
```



<h2 id="installation">âš™ï¸ Installation</h2>

1. Prepare conda environment: `conda create -n smoe python=3.11` (If your environment name is not `smoe`, you may need to change environment in launching scripts)
2. Add correct environment variables in `~/.bashrc` (`gcc` is set to newer version for installing `flash-attn`). e.g.:
     (å¦‚æœæœåŠ¡å™¨ä¸Šæœ‰cudaé©±åŠ¨ï¼Œä¸ç”¨è¿™ä¸€æ­¥å®‰è£…å’Œå†™PATH)
    ```bash
    export PATH=/mnt/petrelfs/share/cuda-11.8/bin:$PATH
    export LD_LIBRARY_PATH=/mnt/petrelfs/share/cuda-11.8/lib64:$LD_LIBRARY_PATH
    export PATH=/mnt/petrelfs/share/gcc-10.1.0/bin:$PATH
    export LD_LIBRARY_PATH=/mnt/petrelfs/share/gcc-10.1.0/lib64:$LD_LIBRARY_PATH
    ```
3. Take the variables into effect: `source ~/.bashrc`
4. Install PyTorch (CUDA-11.8): `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`
5. Install dependencies ï¼ˆrequirementsé‡Œçš„pkgsï¼Œç¡®ä¿transformerså’Œflash-attnçš„ç‰ˆæœ¬ï¼Œå…¶ä»–ç‰ˆæœ¬å¯ä»¥ä¸ä¸€è‡´ï¼‰: `pip install -r requirements.txt`
6. Install `flash-attn`: `pip install flash-attn==2.6.1 --no-build-isolation`. You may need to follow the [flash-attn installation instructions](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) to avoid some errors.
7. Install the latest Git: `conda install git`
8. Clone the repo: `git@github.com:LLaMA-MoE/LLaMA-MoE-v2.git` (If you don't setup the ssh key to GitHub, you may not able to clone through ssh. Check the [docs](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account) about it.)
9. Change current directory: `cd LLaMA-MoE-v2`
10. Install `smoe` in [editable mode](https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-e): `pip install -e .[dev]`
11. Setup `pre-commit` hooks ï¼ˆå¯ä»¥ä¸ç”¨è®¾ç½®ï¼‰: `pre-commit install`




ï¼ˆå…ˆè½¬æ¢å¤åˆ¶modelï¼‰
- LLaMA-MoE 2group 4experts: `bash scripts/expert_construction/convert/convert_mixtral_2group_base.sh`

For more information, please refer to [Expert Construction docs](docs/expert_construction/README.md).


<h2 id="sft">ğŸ’¬ Supervised Fine-Tuning (SFT)</h2>

- è®­ç»ƒå‚æ•°

   ```bash
   python train.py \
   --dataset_save_dir=${data_dir}    æ•°æ®å¤„ç†åä¿å­˜çš„è·¯å¾„ï¼Œå¦‚æœå·²ç»æœ‰å¤„ç†å¥½çš„ï¼Œç›´æ¥åŠ è½½
   --manifest_files=${dataset_dir_or_path}     æ•°æ®å…¨è·¯å¾„æ–‡ä»¶ï¼Œå¤šä¸ªæ–‡ä»¶ç”¨ | éš”å¼€ï¼Œç›®å‰åªèƒ½æ˜¯jsonæˆ–è€…csvæ–‡ä»¶
   --input_fields="src_text|src_text"     æ•°æ®ä¸­æŒ‡å®šçš„å­—æ®µï¼Œä½œä¸ºgptçš„è¾“å…¥
   --output_fields="src_text|tgt_text"     æ•°æ®ä¸­æŒ‡å®šçš„å­—æ®µï¼Œä½œä¸ºgptçš„ç”Ÿæˆ
   --instructions="|"     æŒ‡ä»¤ï¼Œä¸€ä¸ªæ•°æ®æ–‡ä»¶å¯¹åº”ä¸€ä¸ªï¼Œç”¨ | éš”å¼€ï¼Œ lmä»»åŠ¡æ˜¯ç©ºï¼Œæ²¡æœ‰æŒ‡ä»¤ï¼Œç¿»è¯‘ä»»åŠ¡æ¯”å¦‚æ˜¯  "Please translate the English text into Spanish: | Please     translate the English text into French: "
     
   ```

- sft stage_1  lm : `bash scripts/sft/sft_lm_2group_4e_top1_base.sh`
- sft stage_2  st : `bash scripts/sft/sft_mt_2group_4e_top1_base.sh`

- **NOTICE:** Please create `logs/` folder manually: `mkdir -p logs`

  We provide simple examples of SFT to build chatbots. Please refer to [SFT docs](docs/supervised_fine_tuning/LLaMA-MoE-v2.md) for more details.




<h2 id="citation">ğŸ“‘ Citation</h2>

```bibtex
@misc{llama-moe-v2,
  title={LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training},
  author={Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, Yu Cheng},
  year={2024},
  month={Nov},
  url={https://arxiv.org/abs/2411.15708}
}
```

<hr>
<p align="center">LLaMA-MoE Team w/ â¤ï¸</p>
